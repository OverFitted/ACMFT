# Отчет о трансформере адаптивного кросс-модального слияния (ACMFT)

## 1. Введение

Трансформер адаптивного кросс-модального слияния (ACMFT) представляет собой сложную архитектуру глубокого обучения, разработанную преимущественно для распознавания эмоций путем интеграции и интерпретации данных из нескольких модальностей. Она использует визуальные сигналы (мимика), аудиосигналы (характеристики речи) и физиологические данные (паттерны сердечного ритма) для формирования устойчивых и детализированных оценок эмоционального состояния. Такой мультимодальный подход отражает человеческое восприятие эмоций, которое редко опирается на единственный источник информации.

Основное преимущество ACMFT заключается в его способности эффективно обрабатывать каждую модальность, а также обучаться сложным межмодальным зависимостям и адаптивно интегрировать эти разнородные потоки данных. В данном отчете подробно описаны архитектура, ключевые модули, потенциальные области применения и присущие этой конструкции преимущества.

## 2. Обзор архитектуры ACMFT

Модель ACMFT следует структурированному процессу:

1. **Кодирование, специфичное для модальности**: Исходные данные каждой модальности (визуальной, аудио, сердечного ритма) сначала обрабатываются специализированными кодировщиками для извлечения релевантных признаков.
    * **Визуальный кодировщик**: Обрабатывает изображения/видео для захвата черт лица.
    * **Аудиокодировщик**: Обрабатывает аудиозаписи для извлечения речевых и акустических характеристик.
    * **Кодировщик сердечного ритма**: Обрабатывает физиологические сигналы (например, ФПГ, ЭКГ) для получения данных о частоте и вариабельности сердечного ритма.
2. **Встраивание признаков**: Извлеченные признаки каждой модальности, которые могут иметь разные размерности, проецируются в общее, унифицированное пространство встраивания. Этот шаг унифицирует представления признаков, подготавливая их к совместной обработке.
3. **Кросс-модальное слияние**: Это основа ACMFT. Используется серия слоев `CrossModalTransformerBlock`, которые обеспечивают:
    * **Внутримодальное уточнение**: Механизмы само-внимания в рамках встроенных признаков каждой модальности позволяют модели оценивать важность различных частей сигнала этой модальности (например, фокусироваться на определенных временных сегментах аудио или областях изображения).
    * **Межмодальное взаимодействие**: Механизмы кросс-внимания позволяют каждой модальности учитывать и интегрировать информацию из других модальностей. Например, визуальные признаки могут быть скорректированы с учетом аудиосигналов, и наоборот. Это способствует обучению корреляциям и дополнительной информации между модальностями.
4. **Динамическое контекстное управление**: После кросс-модального слияния модуль `DynamicContextualGating` дополнительно уточняет интегрированное представление. Этот модуль:
    * Оценивает «качество» или надежность каждой модальности для данного входного сигнала.
    * Формирует контекстные векторы, суммирующие информационное содержание каждой модальности.
    * Вычисляет веса управления, определяющие вклад каждой модальности в итоговое интегрированное представление на основе контекста и качества. Это позволяет модели адаптивно полагаться на более четкие и релевантные модальности, снижая вес зашумленных или менее информативных.
5. **Классификация эмоций**: Итоговое, адаптивно интегрированное представление передается через классификационную голову (обычно серию полносвязных слоев) для предсказания эмоции.

## 3. Ключевые модули и их функциональность

### 3.1. Кодировщики модальностей

* **Визуальный кодировщик (`models/modality_encoders/visual.py`)**:
  * **Обнаружение лиц**: Использует модель YOLO (You Only Look Once, конкретно `yolo11n.pt`) для обнаружения и обрезки лиц на входных изображениях/кадрах. Это фокусирует анализ на наиболее значимой области для распознавания эмоций по мимике.
  * **Извлечение признаков**: Обрезанные изображения лиц передаются в предварительно обученную глубокую сверточную нейронную сеть. Варианты включают:
    * **FaceNet (InceptionResnetV1)**: Оптимизирована для распознавания лиц, извлекает богатые встраивания, улавливающие тонкие детали лица.
    * **ResNet**: Универсальный мощный экстрактор признаков изображений.
  * Выход представляет собой компактный вектор признаков, описывающий визуальную информацию лица.

* **Аудиокодировщик (`models/modality_encoders/audio.py`)**:
  * **Предобработка**: Сырые аудиоволновые формы стандартизируются путем передискретизации (на общую частоту, например, 16 кГц), корректировки длины (дополнение/обрезка) и нормализации.
  * **Извлечение признаков**: Используется предварительно обученная модель **Wav2Vec2**. Wav2Vec2 обучается контекстуализированным речевым представлениям непосредственно из сырого аудио и высокоэффективна для различных речевых задач.
  * **Агрегация**: Последовательность признаков из Wav2Vec2 агрегируется в единый вектор с использованием методов, таких как усреднение, максимальное объединение или механизм внимания по временным шагам.
  * Выход — вектор признаков, суммирующий релевантные акустические и речевые характеристики.

* **Кодировщик сердечного ритма (`models/modality_encoders/hr.py`)**:
  * **Предобработка**: Сигналы сердечного ритма (например, из ФПГ или ЭКГ) обрабатываются фильтрацией (полосовой фильтр для выделения релевантных частот), передискретизацией, корректировкой длины и нормализацией (Z-оценка).
  * **Извлечение признаков**: Доступны два основных подхода:
    * **1D CNN (`CNNHRFeatureExtractor`)**: Одномерная сверточная нейронная сеть с несколькими ветвями (с разными размерами ядер) обрабатывает волновую форму сердечного ритма для изучения временных паттернов, указывающих на физиологическое состояние.
    * **LSTM (`LSTMHRFeatureExtractor`)**: Сеть на основе LSTM может использоваться для улавливания временных зависимостей в сигнале сердечного ритма.
    * **Признаки ВСР (`HRVFeatureExtractor`)**: Опционально могут вычисляться признаки вариабельности сердечного ритма (ВСР) из RR-интервалов (время между сердечными сокращениями). К ним относятся стандартные метрики во временной области (например, SDNN, RMSSD, pNN50) и частотной области (например, мощность LF, HF, соотношение LF/HF), которые являются признанными индикаторами активности вегетативной нервной системы и стресса.
  * Признаки из анализа волновой формы (CNN/LSTM) и анализа ВСР могут комбинироваться.

### 3.2. Механизмы слияния

* **Встраивание модальности (`models/acmft.py`)**:
  * Простые нейронные сетевые слои (Linear, LayerNorm, Dropout), которые проецируют признаки переменной размерности от каждого кодировщика модальности в фиксированное общее пространство встраивания.

* **Кросс-модальный трансформерный блок (`models/fusion/transformer.py`)**:
  * Основной механизм слияния. Каждый блок включает:
    * **Слои само-внимания**: Применяются независимо к встроенным признакам каждой модальности для уточнения их представлений путем фокусировки на значимых частях внутри этой модальности.
    * **Слои кросс-внимания**: Позволяют каждой модальности запрашивать и интегрировать информацию из других модальностей. Например, визуальное представление может быть улучшено за счет внимания к релевантным частям аудиопредставления, и наоборот для всех пар модальностей. Это позволяет модели изучать прямые межмодальные связи.
    * **Прямые нейронные сети**: Дополнительно обрабатывают представления после внимания.
  * Обычно несколько таких блоков накладываются друг на друга для более глубокого слияния и изучения сложных взаимодействий.

* **Динамическое контекстное управление (`models/fusion/gating.py`)**:
  * **Кодировщик контекста**: Формирует сводку (контекстный вектор) текущего информационного содержания каждой модальности.
  * **Оценщик качества модальности**: Генерирует оценку для каждой модальности, определяя ее надежность или четкость.
  * **Сеть управления**: Принимает контекстные векторы и вычисляет веса управления (суммирующиеся до 1) для каждой модальности.
  * Признаки каждой модальности затем масштабируются их соответствующими весами управления и оценками качества перед объединением (суммированием). Это гарантирует, что итоговое представление ориентировано на модальности, которые являются контекстно значимыми и надежно измеренными.

## 4. Потенциальные области применения

Учитывая архитектуру, ACMFT подходит для множества приложений, особенно тех, которые требуют глубокого понимания человеческих эмоциональных состояний на основе нескольких сигналов:

* **Распознавание эмоций в человеко-машинном взаимодействии (HCI)**: Создание более эмпатичных и адаптивных систем, таких как виртуальные помощники, образовательное ПО или боты для обслуживания клиентов, которые могут адаптироваться к эмоциональному состоянию пользователя.
* **Мониторинг психического здоровья**: Анализ мультимодальных данных (например, из видеозвонков с терапевтами, данных носимых датчиков) для отслеживания эмоционального благополучия, выявления ранних признаков депрессии, тревожности или стресса.
* **Обнаружение сонливости/отвлечения водителя**: Комбинирование мимических сигналов (закрытие глаз, положение головы от визуального кодировщика) с физиологическими сигналами (изменения сердечного ритма от кодировщика сердечного ритма) для повышения безопасности на дорогах.
* **Аффективные вычисления в робототехнике**: Обеспечение социальных роботов способностью воспринимать и адекватно реагировать на человеческие эмоции, что приводит к более естественному и эффективному взаимодействию человека и робота.
* **Исследования рынка и анализ медиа**: Оценка реакций аудитории на рекламу, фильмы или другой медиаконтент путем анализа мимики, тона голоса и физиологических реакций.
* **Здравоохранение**: Мониторинг пациентов (например, оценка боли по мимике и физиологическим сигналам) или улучшение удаленных консультаций, предоставляя клиницистам более полное понимание состояния пациента.
* **Безопасность и обнаружение обмана**: Хотя это более исследовательская область, анализ микро-выражений, вокального стресса и физиологического возбуждения может способствовать разработке систем для оценки достоверности.

## 5. Преимущества подхода ACMFT

* **Устойчивость за счет мультимодальности**:spot: Благодаря интеграции информации из разных источников ACMFT более устойчив к шуму или отсутствию данных в одной из модальностей. Если одна модальность повреждена (например, плохое освещение для визуальной, фоновый шум для аудио), другие могут компенсировать.
* **Комплексное понимание**: Разные модальности часто несут взаимодополняющую или уточняющую информацию об эмоциональных состояниях. Например, улыбка (визуальная) может быть искренней или вежливой, а тон голоса (аудио) или изменения сердечного ритма (сердечный ритм) могут помочь различить. ACMFT разработан для улавливания этих синергетических отношений.
* **Адаптивное слияние**: Механизм `DynamicContextualGating` позволяет модели интеллектуально взвешивать важность каждой модальности на основе конкретного входного контекста и предполагаемого качества сигнала. Это критически важно, поскольку релевантность модальностей может значительно варьироваться в зависимости от ситуации и индивида.
* **Глубокое межмодальное обучение**: `CrossModalTransformerBlock` обеспечивают глубокое взаимодействие между модальностями, выходя за рамки простого объединения или раннего/позднего слияния. Они позволяют модели изучать сложные зависимости и то, как одна модальность модулирует интерпретацию другой.
* **Использование предварительно обученных моделей**: Архитектура позволяет использовать мощные предварительно обученные кодировщики для каждой модальности (YOLO, FaceNet/ResNet для визуальной;cześ

System: Wav2Vec2 для аудио). Это переносит знания из больших наборов данных, улучшая качество признаков и снижая потребность в огромных помеченных наборах данных для конкретной задачи распознавания эмоций.

* **Потенциал интерпретируемости**: Хотя модель сложна, веса внимания из трансформерных блоков и веса управления из модуля динамического контекстного управления могут дать представление о том, на какие модальности и какие части сигналов модель фокусируется при прогнозировании. Это может помочь в понимании и отладке поведения модели.
* **Обработка физиологических данных**: Явное включение кодировщика сердечного ритма позволяет модели использовать физиологические реакции, которые часто менее сознательно контролируются и могут предоставлять объективные подсказки об уровне возбуждения и стресса, дополняющие поведенческие сигналы от лица и голоса.

## 6. Заключение

Архитектура ACMFT представляет собой сложный и мощный подход к мультимодальному распознаванию эмоций и аффективным вычислениям. Ее сочетание специализированных кодировщиков модальностей, глубокого кросс-модального слияния с использованием трансформеров и адаптивного механизма управления позволяет эффективно интегрировать разнообразные потоки данных для более целостного и нюансированного понимания человеческих эмоциональных состояний. Это делает ее перспективным инструментом для широкого спектра приложений, где понимание и реагирование на человеческие эмоции критически важны. Будущие исследования могут изучать расширение на дополнительные модальности, дальнейшие улучшения техник слияния и применение в реальных, динамичных сценариях.
